{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset, get_dataset_config_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_data_subset = get_dataset_config_names(\"xtreme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_data_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "panx_subsets = [s for s in ner_data_subset if s.startswith(\"PAN\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PAN-X.af',\n",
       " 'PAN-X.ar',\n",
       " 'PAN-X.bg',\n",
       " 'PAN-X.bn',\n",
       " 'PAN-X.de',\n",
       " 'PAN-X.el',\n",
       " 'PAN-X.en',\n",
       " 'PAN-X.es',\n",
       " 'PAN-X.et',\n",
       " 'PAN-X.eu',\n",
       " 'PAN-X.fa',\n",
       " 'PAN-X.fi',\n",
       " 'PAN-X.fr',\n",
       " 'PAN-X.he',\n",
       " 'PAN-X.hi',\n",
       " 'PAN-X.hu',\n",
       " 'PAN-X.id',\n",
       " 'PAN-X.it',\n",
       " 'PAN-X.ja',\n",
       " 'PAN-X.jv',\n",
       " 'PAN-X.ka',\n",
       " 'PAN-X.kk',\n",
       " 'PAN-X.ko',\n",
       " 'PAN-X.ml',\n",
       " 'PAN-X.mr',\n",
       " 'PAN-X.ms',\n",
       " 'PAN-X.my',\n",
       " 'PAN-X.nl',\n",
       " 'PAN-X.pt',\n",
       " 'PAN-X.ru',\n",
       " 'PAN-X.sw',\n",
       " 'PAN-X.ta',\n",
       " 'PAN-X.te',\n",
       " 'PAN-X.th',\n",
       " 'PAN-X.tl',\n",
       " 'PAN-X.tr',\n",
       " 'PAN-X.ur',\n",
       " 'PAN-X.vi',\n",
       " 'PAN-X.yo',\n",
       " 'PAN-X.zh']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "panx_subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Shivansh\\\\iNeuron\\\\Projects\\\\Class Projects\\\\NLP\\\\Name-Entity-Recognization'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m en \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mxtreme\u001b[39;49m\u001b[39m\"\u001b[39;49m, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPAN-X.en\u001b[39;49m\u001b[39m\"\u001b[39;49m, cache_dir\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39martifacts/cache_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mg:\\Shivansh\\iNeuron\\Projects\\Class Projects\\NLP\\Name-Entity-Recognization\\env\\lib\\site-packages\\datasets\\load.py:1718\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[0;32m   1715\u001b[0m ignore_verifications \u001b[39m=\u001b[39m ignore_verifications \u001b[39mor\u001b[39;00m save_infos\n\u001b[0;32m   1717\u001b[0m \u001b[39m# Create a dataset builder\u001b[39;00m\n\u001b[1;32m-> 1718\u001b[0m builder_instance \u001b[39m=\u001b[39m load_dataset_builder(\n\u001b[0;32m   1719\u001b[0m     path\u001b[39m=\u001b[39;49mpath,\n\u001b[0;32m   1720\u001b[0m     name\u001b[39m=\u001b[39;49mname,\n\u001b[0;32m   1721\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[0;32m   1722\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[0;32m   1723\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1724\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   1725\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[0;32m   1726\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[0;32m   1727\u001b[0m     revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[0;32m   1728\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1729\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[0;32m   1730\u001b[0m )\n\u001b[0;32m   1732\u001b[0m \u001b[39m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[0;32m   1733\u001b[0m \u001b[39mif\u001b[39;00m streaming:\n",
      "File \u001b[1;32mg:\\Shivansh\\iNeuron\\Projects\\Class Projects\\NLP\\Name-Entity-Recognization\\env\\lib\\site-packages\\datasets\\load.py:1514\u001b[0m, in \u001b[0;36mload_dataset_builder\u001b[1;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\u001b[0m\n\u001b[0;32m   1511\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(error_msg)\n\u001b[0;32m   1513\u001b[0m \u001b[39m# Instantiate the dataset builder\u001b[39;00m\n\u001b[1;32m-> 1514\u001b[0m builder_instance: DatasetBuilder \u001b[39m=\u001b[39m builder_cls(\n\u001b[0;32m   1515\u001b[0m     cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[0;32m   1516\u001b[0m     config_name\u001b[39m=\u001b[39;49mconfig_name,\n\u001b[0;32m   1517\u001b[0m     data_dir\u001b[39m=\u001b[39;49mdata_dir,\n\u001b[0;32m   1518\u001b[0m     data_files\u001b[39m=\u001b[39;49mdata_files,\n\u001b[0;32m   1519\u001b[0m     \u001b[39mhash\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mhash\u001b[39;49m,\n\u001b[0;32m   1520\u001b[0m     features\u001b[39m=\u001b[39;49mfeatures,\n\u001b[0;32m   1521\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[0;32m   1522\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbuilder_kwargs,\n\u001b[0;32m   1523\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig_kwargs,\n\u001b[0;32m   1524\u001b[0m )\n\u001b[0;32m   1526\u001b[0m \u001b[39mreturn\u001b[39;00m builder_instance\n",
      "File \u001b[1;32mg:\\Shivansh\\iNeuron\\Projects\\Class Projects\\NLP\\Name-Entity-Recognization\\env\\lib\\site-packages\\datasets\\builder.py:1305\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder.__init__\u001b[1;34m(self, writer_batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, writer_batch_size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m-> 1305\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1306\u001b[0m     \u001b[39m# Batch size used by the ArrowWriter\u001b[39;00m\n\u001b[0;32m   1307\u001b[0m     \u001b[39m# It defines the number of samples that are kept in memory before writing them\u001b[39;00m\n\u001b[0;32m   1308\u001b[0m     \u001b[39m# and also the length of the arrow chunks\u001b[39;00m\n\u001b[0;32m   1309\u001b[0m     \u001b[39m# None means that the ArrowWriter will use its default value\u001b[39;00m\n\u001b[0;32m   1310\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_writer_batch_size \u001b[39m=\u001b[39m writer_batch_size \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mDEFAULT_WRITER_BATCH_SIZE\n",
      "File \u001b[1;32mg:\\Shivansh\\iNeuron\\Projects\\Class Projects\\NLP\\Name-Entity-Recognization\\env\\lib\\site-packages\\datasets\\builder.py:351\u001b[0m, in \u001b[0;36mDatasetBuilder.__init__\u001b[1;34m(self, cache_dir, config_name, hash, base_path, info, features, use_auth_token, repo_id, data_files, data_dir, name, **config_kwargs)\u001b[0m\n\u001b[0;32m    349\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir_root, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    350\u001b[0m lock_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir_root, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir\u001b[39m.\u001b[39mreplace(os\u001b[39m.\u001b[39msep, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 351\u001b[0m \u001b[39mwith\u001b[39;00m FileLock(lock_path):\n\u001b[0;32m    352\u001b[0m     \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir):  \u001b[39m# check if data exist\u001b[39;00m\n\u001b[0;32m    353\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(os\u001b[39m.\u001b[39mlistdir(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache_dir)) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mg:\\Shivansh\\iNeuron\\Projects\\Class Projects\\NLP\\Name-Entity-Recognization\\env\\lib\\site-packages\\datasets\\utils\\filelock.py:320\u001b[0m, in \u001b[0;36mBaseFileLock.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__enter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 320\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    321\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mg:\\Shivansh\\iNeuron\\Projects\\Class Projects\\NLP\\Name-Entity-Recognization\\env\\lib\\site-packages\\datasets\\utils\\filelock.py:281\u001b[0m, in \u001b[0;36mBaseFileLock.acquire\u001b[1;34m(self, timeout, poll_intervall)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m             logger()\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    279\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLock \u001b[39m\u001b[39m{\u001b[39;00mlock_id\u001b[39m}\u001b[39;00m\u001b[39m not acquired on \u001b[39m\u001b[39m{\u001b[39;00mlock_filename\u001b[39m}\u001b[39;00m\u001b[39m, waiting \u001b[39m\u001b[39m{\u001b[39;00mpoll_intervall\u001b[39m}\u001b[39;00m\u001b[39m seconds ...\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m             )\n\u001b[1;32m--> 281\u001b[0m             time\u001b[39m.\u001b[39;49msleep(poll_intervall)\n\u001b[0;32m    282\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     \u001b[39m# Something did go wrong, so decrement the counter.\u001b[39;00m\n\u001b[0;32m    284\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_thread_lock:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "en = load_dataset(\"xtreme\", name=\"PAN-X.en\", cache_dir=\"artifacts/cache_dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.15 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f6c33e3165c0a1a212178ade3054181477fddf9002698c2c2065fc3f703b291"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
